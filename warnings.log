WARNING:root:With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator
WARNING:root:Coordinate descent with no regularization may lead to unexpected results and is discouraged.
WARNING:root:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.106e+01, tolerance: 7.361e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.
WARNING:root:With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator
WARNING:root:Coordinate descent with no regularization may lead to unexpected results and is discouraged.
WARNING:root:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.376e+01, tolerance: 4.015e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.
WARNING:root:With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator
WARNING:root:Coordinate descent with no regularization may lead to unexpected results and is discouraged.
WARNING:root:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.997e+00, tolerance: 1.291e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.
WARNING:root:With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator
WARNING:root:Coordinate descent with no regularization may lead to unexpected results and is discouraged.
WARNING:root:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.106e+01, tolerance: 7.361e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.
WARNING:root:With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator
WARNING:root:Coordinate descent with no regularization may lead to unexpected results and is discouraged.
WARNING:root:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.376e+01, tolerance: 4.015e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.
WARNING:root:With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator
WARNING:root:Coordinate descent with no regularization may lead to unexpected results and is discouraged.
WARNING:root:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.997e+00, tolerance: 1.291e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.
WARNING:root:With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator
WARNING:root:Coordinate descent with no regularization may lead to unexpected results and is discouraged.
WARNING:root:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.106e+01, tolerance: 7.361e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.
WARNING:root:With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator
WARNING:root:Coordinate descent with no regularization may lead to unexpected results and is discouraged.
WARNING:root:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.376e+01, tolerance: 4.015e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.
WARNING:root:With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator
WARNING:root:Coordinate descent with no regularization may lead to unexpected results and is discouraged.
WARNING:root:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.997e+00, tolerance: 1.291e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.
WARNING:root:With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator
WARNING:root:Coordinate descent with no regularization may lead to unexpected results and is discouraged.
WARNING:root:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.106e+01, tolerance: 7.361e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.
WARNING:root:With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator
WARNING:root:Coordinate descent with no regularization may lead to unexpected results and is discouraged.
WARNING:root:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.376e+01, tolerance: 4.015e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.
WARNING:root:With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator
WARNING:root:Coordinate descent with no regularization may lead to unexpected results and is discouraged.
WARNING:root:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.997e+00, tolerance: 1.291e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.
WARNING:root:With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator
WARNING:root:Coordinate descent with no regularization may lead to unexpected results and is discouraged.
WARNING:root:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.106e+01, tolerance: 7.361e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.
WARNING:root:With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator
WARNING:root:Coordinate descent with no regularization may lead to unexpected results and is discouraged.
WARNING:root:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.376e+01, tolerance: 4.015e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.
WARNING:root:With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator
WARNING:root:Coordinate descent with no regularization may lead to unexpected results and is discouraged.
WARNING:root:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.997e+00, tolerance: 1.291e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.
WARNING:root:With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator
WARNING:root:Coordinate descent with no regularization may lead to unexpected results and is discouraged.
WARNING:root:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.874e+01, tolerance: 4.527e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.
WARNING:root:With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator
WARNING:root:Coordinate descent with no regularization may lead to unexpected results and is discouraged.
WARNING:root:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.246e+00, tolerance: 2.240e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.
WARNING:root:With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator
WARNING:root:Coordinate descent with no regularization may lead to unexpected results and is discouraged.
WARNING:root:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.728e+00, tolerance: 5.333e-04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.
WARNING:root:With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator
WARNING:root:Coordinate descent with no regularization may lead to unexpected results and is discouraged.
WARNING:root:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.874e+01, tolerance: 4.527e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.
WARNING:root:With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator
WARNING:root:Coordinate descent with no regularization may lead to unexpected results and is discouraged.
WARNING:root:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.246e+00, tolerance: 2.240e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.
WARNING:root:With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator
WARNING:root:Coordinate descent with no regularization may lead to unexpected results and is discouraged.
WARNING:root:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.728e+00, tolerance: 5.333e-04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.
WARNING:root:With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator
WARNING:root:Coordinate descent with no regularization may lead to unexpected results and is discouraged.
WARNING:root:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.874e+01, tolerance: 4.527e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.
WARNING:root:With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator
WARNING:root:Coordinate descent with no regularization may lead to unexpected results and is discouraged.
WARNING:root:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.246e+00, tolerance: 2.240e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.
WARNING:root:With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator
WARNING:root:Coordinate descent with no regularization may lead to unexpected results and is discouraged.
WARNING:root:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.728e+00, tolerance: 5.333e-04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.
WARNING:root:With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator
WARNING:root:Coordinate descent with no regularization may lead to unexpected results and is discouraged.
WARNING:root:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.874e+01, tolerance: 4.527e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.
WARNING:root:With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator
WARNING:root:Coordinate descent with no regularization may lead to unexpected results and is discouraged.
WARNING:root:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.246e+00, tolerance: 2.240e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.
WARNING:root:With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator
WARNING:root:Coordinate descent with no regularization may lead to unexpected results and is discouraged.
WARNING:root:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.728e+00, tolerance: 5.333e-04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.
WARNING:root:With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator
WARNING:root:Coordinate descent with no regularization may lead to unexpected results and is discouraged.
WARNING:root:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.874e+01, tolerance: 4.527e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.
WARNING:root:With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator
WARNING:root:Coordinate descent with no regularization may lead to unexpected results and is discouraged.
WARNING:root:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.246e+00, tolerance: 2.240e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.
WARNING:root:With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator
WARNING:root:Coordinate descent with no regularization may lead to unexpected results and is discouraged.
WARNING:root:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.728e+00, tolerance: 5.333e-04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.
WARNING:root:With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator
WARNING:root:Coordinate descent with no regularization may lead to unexpected results and is discouraged.
WARNING:root:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.874e+01, tolerance: 4.527e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.
WARNING:root:With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator
WARNING:root:Coordinate descent with no regularization may lead to unexpected results and is discouraged.
WARNING:root:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.246e+00, tolerance: 2.240e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.
WARNING:root:With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator
WARNING:root:Coordinate descent with no regularization may lead to unexpected results and is discouraged.
WARNING:root:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.728e+00, tolerance: 5.333e-04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.
WARNING:root:With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator
WARNING:root:Coordinate descent with no regularization may lead to unexpected results and is discouraged.
WARNING:root:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.874e+01, tolerance: 4.527e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.
WARNING:root:With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator
WARNING:root:Coordinate descent with no regularization may lead to unexpected results and is discouraged.
WARNING:root:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.246e+00, tolerance: 2.240e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.
WARNING:root:With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator
WARNING:root:Coordinate descent with no regularization may lead to unexpected results and is discouraged.
WARNING:root:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.728e+00, tolerance: 5.333e-04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.
WARNING:root:With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator
WARNING:root:Coordinate descent with no regularization may lead to unexpected results and is discouraged.
WARNING:root:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.874e+01, tolerance: 4.527e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.
WARNING:root:With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator
WARNING:root:Coordinate descent with no regularization may lead to unexpected results and is discouraged.
WARNING:root:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.246e+00, tolerance: 2.240e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.
WARNING:root:With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator
WARNING:root:Coordinate descent with no regularization may lead to unexpected results and is discouraged.
WARNING:root:Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.728e+00, tolerance: 5.333e-04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.
