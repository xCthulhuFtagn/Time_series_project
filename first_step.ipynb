{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from dateutil.parser import parse\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "import matplotlib as mpl\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import scipy.stats as scs\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "plt.rcParams.update({'figure.figsize': (15, 7), 'figure.dpi': 120})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Идеи для фич"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) лаги/средние/...\n",
    "2) курс доллара\n",
    "3) дата налогового дня?\n",
    "4) праздники\n",
    "5) время отдыха\n",
    "6) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"/home/owner/Documents/DEV/Time_series/Project 1_2024.xlsx\", parse_dates=['Date'], index_col='Date', usecols=['Balance', 'Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# График временного ряда"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Разбиение данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = df.index[0]\n",
    "threshold_date = np.datetime64('2019-07-30')\n",
    "end_date = df.index[-1]#np.datetime64('2020-09-11')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.index <= end_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_before = (df.index < threshold_date) & (df.index >= start_date)\n",
    "mask_after = (df.index >= threshold_date) & (df.index <= end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тест Дики-Фуллера для проверки на стационарность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_stationarity(series):\n",
    "    # Copied from https://machinelearningmastery.com/time-series-data-stationary-python/\n",
    "\n",
    "    result = adfuller(series.values)\n",
    "\n",
    "    print('ADF Statistic: %f' % result[0])\n",
    "    print('p-value: %f' % result[1])\n",
    "    print('Critical Values:')\n",
    "    for key, value in result[4].items():\n",
    "        print('\\t%s: %.3f' % (key, value))\n",
    "\n",
    "    if (result[1] <= 0.05) & (result[4]['5%'] > result[0]):\n",
    "        print(\"\\u001b[32mStationary\\u001b[0m\")\n",
    "    else:\n",
    "        print(\"\\x1b[31mNon-stationary\\x1b[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_stationarity(df.Balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_stationarity(df.Balance[mask_before])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_stationarity(df.Balance[mask_after])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://forecastegy.com/posts/change-point-detection-time-series-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тут реализована модификация статистики взвешенного экспоненциального среднего.\n",
    "class MeanExpNoDataException(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class MeanExp(object):\n",
    "    def __init__(self, new_value_weight, load_function=np.median):\n",
    "        self._load_function = load_function\n",
    "        self._new_value_weight = new_value_weight\n",
    "        self.load([])\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        if self._weights_sum <= 1:\n",
    "            raise MeanExpNoDataException('self._weights_sum <= 1')\n",
    "        return self._values_sum / self._weights_sum\n",
    "\n",
    "    def update(self, new_value, **kwargs):\n",
    "        self._values_sum = (1 - self._new_value_weight) * self._values_sum + new_value\n",
    "        self._weights_sum = (1 - self._new_value_weight) * self._weights_sum + 1.0\n",
    "\n",
    "    def load(self, old_values):\n",
    "        if old_values:\n",
    "            old_values = [value for ts, value in old_values]\n",
    "            mean = float(self._load_function(old_values))\n",
    "            self._weights_sum = min(float(len(old_values)), 1.0 / self._new_value_weight)\n",
    "            self._values_sum = mean * self._weights_sum\n",
    "        else:\n",
    "            self._values_sum = 0.0\n",
    "            self._weights_sum = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stat(object):\n",
    "    def __init__(self, threshold, direction=\"unknown\", init_stat=0.0):\n",
    "        self._direction = str(direction)\n",
    "        self._threshold = float(threshold)\n",
    "        self._stat = float(init_stat)\n",
    "        self._alarm = self._stat / self._threshold\n",
    "    \n",
    "    @property\n",
    "    def direction(self):\n",
    "        return self._direction\n",
    "\n",
    "    @property\n",
    "    def stat(self):\n",
    "        return self._stat\n",
    "        \n",
    "    @property\n",
    "    def alarm(self):\n",
    "        return self._alarm\n",
    "        \n",
    "    @property\n",
    "    def threshold(self):\n",
    "        return self._threshold\n",
    "    \n",
    "    def update(self, **kwargs):\n",
    "        # Statistics may use any of the following kwargs:\n",
    "        #   ts - timestamp for the value\n",
    "        #   value - original value\n",
    "        #   mean - current estimated mean\n",
    "        #   std - current estimated std\n",
    "        #   adjusted_value - usually (value - mean) / std\n",
    "        # Statistics call this after updating '_stat'\n",
    "        self._alarm = self._stat / self._threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdjustedShiryaevRoberts(Stat):\n",
    "    def __init__(self, mean_diff, threshold, max_stat=float(\"+inf\"), init_stat=0.0):\n",
    "        super(AdjustedShiryaevRoberts, self).__init__(threshold,\n",
    "                                                      direction=\"up\",\n",
    "                                                      init_stat=init_stat)\n",
    "        self._mean_diff = mean_diff\n",
    "        self._max_stat = max_stat\n",
    "\n",
    "    def update(self, adjusted_value, **kwargs):\n",
    "        likelihood = np.exp(self._mean_diff * (adjusted_value - self._mean_diff / 2.))\n",
    "        self._stat = min(self._max_stat, (1. + self._stat) * likelihood)\n",
    "        Stat.update(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "beta = 0.05\n",
    "sigma_diff = 2.0\n",
    "\n",
    "stat_trajectory, mean_values, var_values, diff_values = [], [], [], []\n",
    "timestamps, values, changepoint = [], [], []\n",
    "\n",
    "mean_exp = MeanExp(new_value_weight=alpha)\n",
    "var_exp = MeanExp(new_value_weight=beta)\n",
    "sr = AdjustedShiryaevRoberts(sigma_diff, 1000., max_stat=1e9)\n",
    "for ts, x_k in zip(df.index, df.Balance):\n",
    "    timestamps.append(ts)\n",
    "    values.append(x_k)\n",
    "    try:\n",
    "        mean_estimate = mean_exp.value\n",
    "    except MeanExpNoDataException:\n",
    "        mean_estimate = 0.\n",
    "    \n",
    "    try:\n",
    "        var_estimate = var_exp.value\n",
    "    except MeanExpNoDataException:\n",
    "        var_estimate = 1.\n",
    "    \n",
    "    predicted_diff_value = (x_k - mean_estimate) ** 2\n",
    "    predicted_diff_mean = var_estimate\n",
    "    sr.update(predicted_diff_value - predicted_diff_mean)\n",
    "    diff_values.append(predicted_diff_value - predicted_diff_mean)\n",
    "    \n",
    "    mean_exp.update(x_k)\n",
    "    diff_value = (x_k - mean_estimate) ** 2\n",
    "    var_exp.update(diff_value)\n",
    "    \n",
    "    stat_trajectory.append(sr._stat)\n",
    "    mean_values.append(mean_estimate)\n",
    "    var_values.append(np.sqrt(var_estimate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(values)\n",
    "plt.plot(np.array(mean_values), 'k')\n",
    "plt.plot(np.array(mean_values) + np.sqrt(var_values), 'k')\n",
    "plt.plot(np.array(mean_values) - np.sqrt(var_values), 'k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(diff_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.semilogy(stat_trajectory)\n",
    "plt.grid('on')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Избавляемся от выбросов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x = df.index, y = df.Balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore(s, window, thresh=3, return_all=False):\n",
    "    roll = s.rolling(window=window, min_periods=1, center=True)\n",
    "    avg = roll.mean()\n",
    "    std = roll.std(ddof=0)\n",
    "    z = s.sub(avg).div(std)   \n",
    "    m = z.between(-thresh, thresh)\n",
    "    \n",
    "    if return_all:\n",
    "        return z, avg, std, m\n",
    "    return s.where(m, avg)\n",
    "\n",
    "Balance_balanced = zscore(df.Balance, window=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(df.Balance, color='red')\n",
    "sns.lineplot(Balance_balanced, color='green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z, avg, std, m = zscore(df.Balance, window=70, return_all=True)\n",
    "\n",
    "ax = plt.subplot()\n",
    "\n",
    "df.Balance.plot(label='data')\n",
    "avg.plot(label='mean')\n",
    "df.loc[~m, 'Balance'].plot(label='outliers', marker='o', ls='')\n",
    "avg[~m].plot(label='replacement', marker='o', ls='')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Замену можно провести, все найс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Balance = Balance_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(df.Balance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/75938497/outlier-detection-of-time-series-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Разложение на компоненты (попытка)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additive Decomposition\n",
    "result_add = seasonal_decompose(df.Balance, model='additive', extrapolate_trend='freq')\n",
    "\n",
    "# Plot\n",
    "plt.rcParams.update({'figure.figsize': (10,10)})\n",
    "result_add.plot()\n",
    "seasonal = result_add.seasonal\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BalanceD1 = (df.Balance - df.Balance.shift(7))[7:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additive Decomposition\n",
    "result_add2 = seasonal_decompose(BalanceD1, model='additive', extrapolate_trend='freq')\n",
    "\n",
    "# Plot\n",
    "plt.rcParams.update({'figure.figsize': (10,10)})\n",
    "result_add2.plot()\n",
    "seasonal2 = result_add2.seasonal\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(seasonal)\n",
    "sns.lineplot(seasonal2, color='orange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сезонная компонента уменьшена"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BalanceD2 = (BalanceD1 - BalanceD1.shift(7))[7:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additive Decomposition\n",
    "result_add3 = seasonal_decompose(BalanceD2, model='additive', extrapolate_trend='freq')\n",
    "\n",
    "# Plot\n",
    "plt.rcParams.update({'figure.figsize': (10,10)})\n",
    "result_add3.plot()\n",
    "seasonal3 = result_add2.seasonal\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(seasonal2, color='green')\n",
    "sns.lineplot(seasonal3, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Толковых изменений нет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Стабилизация Дисперсии (?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Изучение ACF/PACF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsplot(y, lags=None, figsize=(15, 15), style='bmh'):\n",
    "    if not isinstance(y, pd.Series):\n",
    "        y = pd.Series(y)\n",
    "    with plt.style.context(style):    \n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        #mpl.rcParams['font.family'] = 'Ubuntu Mono'\n",
    "        layout = (3, 2)\n",
    "        ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)\n",
    "        acf_ax = plt.subplot2grid(layout, (1, 0))\n",
    "        pacf_ax = plt.subplot2grid(layout, (1, 1))\n",
    "        qq_ax = plt.subplot2grid(layout, (2, 0))\n",
    "        pp_ax = plt.subplot2grid(layout, (2, 1))\n",
    "        \n",
    "        y.plot(ax=ts_ax)\n",
    "        ts_ax.set_title('Time Series Analysis Plots')\n",
    "        \n",
    "        sm.graphics.tsa.plot_acf(y, lags=lags, ax=acf_ax, alpha=0.05)\n",
    "        sm.graphics.tsa.plot_pacf(y, lags=lags, ax=pacf_ax, alpha=0.05)\n",
    "        sm.qqplot(y, line='s', ax=qq_ax)\n",
    "        qq_ax.set_title('QQ Plot')        \n",
    "        scs.probplot(y, sparams=(y.mean(), y.std()), plot=pp_ax)\n",
    "\n",
    "        plt.tight_layout()\n",
    "    return\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "# plot of discrete white noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsplot(df.Balance, lags=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsplot(BalanceD1, lags=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.graphics.tsa.plot_acf(BalanceD1, lags=50, alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.graphics.tsa.plot_pacf(BalanceD1, lags=50, alpha=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Похоже на SARIMA(p=2, i=0, q=2, P=5, I=0, Q=1, S=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Моделирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "# Define the range of values for p, d, q, P, D, Q, and m\n",
    "p_values = range(0, 4)  # Autoregressive order\n",
    "d_values = [0]          # Differencing order\n",
    "q_values = range(0, 4)  # Moving average order\n",
    "P_values = range(0, 5) # Seasonal autoregressive order\n",
    "D_values = [1]  # Seasonal differencing order\n",
    "Q_values = range(0, 3)  # Seasonal moving average order\n",
    "m_values = [7]         # Seasonal period\n",
    "\n",
    "# Create all possible combinations of SARIMA parameters\n",
    "param_combinations = list(itertools.product(p_values, \n",
    "                                            d_values, \n",
    "                                            q_values, \n",
    "                                            P_values, \n",
    "                                            D_values, \n",
    "                                            Q_values, \n",
    "                                            m_values))\n",
    "results_exog = []\n",
    "\n",
    "# Initialize AIC with a large value\n",
    "best_aic_SARIMA = float(\"inf\")  \n",
    "best_params_SARIMAX = None\n",
    "best_SARIMA = None\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Perform grid search\n",
    "for params in tqdm(param_combinations):\n",
    "    order = params[:3]\n",
    "    seasonal_order = params[3:]\n",
    "    \n",
    "    try:\n",
    "        model_chosen = ARIMA(endog=df[mask_before].Balance, \n",
    "                               order=order, \n",
    "                               seasonal_order=seasonal_order)\n",
    "        model_chosen = model_chosen.fit()\n",
    "        aic = model_chosen.aic\n",
    "        \n",
    "        # Ensure the convergence of the model\n",
    "        if not math.isinf(model_chosen.zvalues.mean()):\n",
    "            predict=model_chosen.predict(start=threshold_date, end=end_date, exog=None)\n",
    "            err_max = (df.Balance[mask_after]-predict).abs().max()\n",
    "            mae = mean_absolute_error(df.Balance[mask_after], predict)\n",
    "            llf = model_chosen.llf\n",
    "            r2 = r2_score(df.Balance[mask_after], predict)\n",
    "            aic = model_chosen.aic\n",
    "            if aic < best_aic_SARIMA:\n",
    "                best_aic_SARIMA = aic\n",
    "                best_params_SARIMA = params\n",
    "                best_SARIMA = model_chosen\n",
    "                best_err_max = err_max\n",
    "                best_mae = mae\n",
    "                best_llf = llf\n",
    "                best_r2 = r2\n",
    "                best_combination = params\n",
    "            results_exog.append(list(params[:-1]) + [err_max, mae, llf, r2, aic])\n",
    "        else:\n",
    "            print(order, seasonal_order, 'not converged')\n",
    "    except ValueError:\n",
    "        print('wrong parameters:', params)\n",
    "        continue\n",
    "\n",
    "# Print the best parameters and AIC\n",
    "print(\"Best Parameters:\", best_params_SARIMA)\n",
    "print(\"Best AIC:\", best_aic_SARIMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_table = pd.DataFrame(results_exog)\n",
    "result_table.columns = ['p', 'd', 'q', 'P', 'D', 'Q', 'err_max', 'MAE', 'LLF', 'R2', 'aic']\n",
    "print(\"by LLF:\")\n",
    "display(result_table.sort_values(by=['LLF'], ascending=False).head(1))\n",
    "print(\"by err_max:\")\n",
    "display(result_table.sort_values(by=['err_max'], ascending=True).head(1))\n",
    "print(\"by MAE:\")\n",
    "display(result_table.sort_values(by=['MAE'], ascending=True).head(1))\n",
    "print(\"by aic:\")\n",
    "display(result_table.sort_values(by=['aic'], ascending=True).head(1))\n",
    "print(\"by R2:\")\n",
    "display(result_table.sort_values(by=['R2'], ascending=False).head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_SARIMA.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmdarima.datasets import load_lynx\n",
    "from pmdarima.arima.utils import nsdiffs\n",
    "\n",
    "# estimate number of seasonal differences using a Canova-Hansen test\n",
    "D = nsdiffs(df.Balance,\n",
    "            m=10,  # commonly requires knowledge of dataset\n",
    "            max_D=12,\n",
    "            test='ch')  # -> 0\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pmdarima as pm\n",
    "best_auto = pm.AutoARIMA(\n",
    "                        # test='adf',\n",
    "                        # start_p=0, max_p=4,\n",
    "                        # d=0,\n",
    "                        # start_q=0, max_q=4,\n",
    "                        # start_P=0, max_P=5,\n",
    "                        start_D=0, max_D=2,\n",
    "                        # start_Q=0, max_Q=5,\n",
    "                        m=7,\n",
    "                        seasonal=True,\n",
    "                        stationary=True,\n",
    "                        trace=True,\n",
    "                        suppress_warnings=True,\n",
    "                        error_action='ignore',\n",
    "                        stepwise=True\n",
    "                      ).fit(y=df.Balance[mask_before])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_auto.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Максимальная и средняя ошибки для выбранной авто:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_auto=best_auto.predict_in_sample(start=threshold_date, end=end_date)\n",
    "err_auto = (df.Balance[mask_after]-predict_auto).abs()\n",
    "print(err_auto.max(), err_auto.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Максимальная и средняя ошибки для выбранной перебором:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_chosen = best_SARIMA.predict(start=threshold_date, end=end_date)\n",
    "err_chosen = (df.Balance[mask_after]-predict_chosen).abs()\n",
    "print(err_chosen.max(), err_chosen.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=df.index, y=df.Balance, label='test', color = 'purple')\n",
    "sns.lineplot(x=df.index, y=best_SARIMA.predict(start=start_date, end=end_date), label='pred_chosen', linestyle='dashed', color='red')\n",
    "sns.lineplot(x=df.index[mask_after], y=predict_auto, label='pred_auto', linestyle='dashed', color='black')\n",
    "\n",
    "plt.axvline(x=threshold_date, color='limegreen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=df.index[mask_after], y=(df.Balance[mask_after]-predict_auto).abs(), label='auto', color = 'green')\n",
    "sns.lineplot(x=df.index[mask_after], y=(df.Balance[mask_after]-predict_chosen).abs(), label='chosen', color = 'red')\n",
    "plt.axhline(y=0.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"mean chosen = {(df.Balance[mask_after]-predict_chosen).abs().mean()}\")\n",
    "print(f\"max chosen = {(df.Balance[mask_after]-predict_chosen).abs().max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предположим мы взяли best_SARIMA модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Посмотрим как ведут себя остатки**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_add = seasonal_decompose((best_SARIMA.resid), model='additive', extrapolate_trend='freq')\n",
    "\n",
    "plt.rcParams.update({'figure.figsize': (10,10)})\n",
    "result_add.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARCH/GARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsplot(best_SARIMA.resid, lags=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/tej-api-financial-data-anlaysis/data-analysis-10-arima-garch-model-part-1-a011bf45f66c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_SARIMA.plot_diagnostics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим являются ли остатки белым шумом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "arima_resid = best_SARIMA.resid\n",
    "white_noise_arima = acorr_ljungbox(arima_resid, lags = [10], return_df=True)\n",
    "white_noise_arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.diagnostic import het_arch\n",
    "LM_pvalue = het_arch(best_SARIMA.resid, ddof = 4)[1]\n",
    "print('LM-test-Pvalue:', '{:.5f}'.format(LM_pvalue))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Т.к. p-value < 0.05, применим GARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsplot(best_SARIMA.resid**2, lags=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пики на первом и седьмом шифте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arch import arch_model\n",
    "\n",
    "ps = range(1,15)\n",
    "qs = range(1,15)\n",
    "\n",
    "params_variants = list(itertools.product(ps, qs))\n",
    "results = []\n",
    "\n",
    "for p, q in tqdm(params_variants):\n",
    "    am = arch_model(best_SARIMA.resid, p=p, q=q).fit(update_freq=0)\n",
    "    \n",
    "    result = {\n",
    "        'p': p,\n",
    "        'q': q,\n",
    "        'AIC': am.aic,\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "best_model_aic = results_df.loc[results_df['AIC'].idxmin()]\n",
    "\n",
    "display(\"Best GARCH model (AIC):\")\n",
    "display(best_model_aic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 8\n",
    "q = 8\n",
    "am = arch_model(best_SARIMA.resid, p=p, q=q).fit()\n",
    "fig = am.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsplot(am.resid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "garch_resid = am.resid\n",
    "white_noise_garch = acorr_ljungbox(garch_resid, lags = [10], return_df=True)\n",
    "white_noise_garch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(best_SARIMA.resid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(garch_resid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P-value нормальные вроде, тогда берем в расход"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/analytics-vidhya/arima-garch-forecasting-with-python-7a3f797de3ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_volatility = am.forecast(start=threshold_date-1, horizon=pd.Timedelta(end_date-threshold_date).days+1)\n",
    "forecast_volatility_values = forecast_volatility.variance.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_bound = predict_chosen + np.sqrt(forecast_volatility_values)\n",
    "lower_bound = predict_chosen - np.sqrt(forecast_volatility_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_volatility.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "sns.lineplot(x=df.index[mask_after], y=predict_chosen, label='pred_chosen', linestyle='dashed', color='red', ax=ax)\n",
    "\n",
    "\n",
    "sns.lineplot(x=df.index[mask_after], y=upper_bound, label='bounds', color = 'magenta')\n",
    "sns.lineplot(x=df.index[mask_after], y=lower_bound, label='bounds', color = 'magenta')\n",
    "sns.lineplot(x=df.index[mask_after], y=df.Balance[mask_after], label='test', color = 'darkgreen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_stationarity(df.Balance[mask_after])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Попробуем зафигачить фичи дополнительные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Курс доллара"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.index[0], df.index[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USD_RUB_df = pd.read_csv(\n",
    "    \"/home/owner/Documents/DEV/Time_series/USD_RUB.csv\",\n",
    "    index_col='Дата', \n",
    "    parse_dates=['Дата'],\n",
    "    usecols=['Дата', 'Цена'],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USD_RUB_df.rename(columns={'Цена' : 'USD'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.join(USD_RUB_df)\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while not new_df.USD.notna().all():\n",
    "    new_df.fillna(new_df.shift(1), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.USD = new_df.USD.str.replace(',', '.').astype(float)\n",
    "new_df.USD = new_df.USD.shift(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ВВП РФ по годам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GDP = {2016: 1.28, 2017: 1.57, 2018: 1.66, 2019: 1.69, 2020: 1.49, 2021: 1.84} # in trillions\n",
    "\n",
    "new_df['last_year_GDP'] = (new_df.index.year - 1)\n",
    "new_df['last_year_GDP'] = new_df['last_year_GDP'].apply(lambda x: GDP[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ВВП РФ по кварталам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GDP = pd.read_excel(\"/home/owner/Documents/DEV/Time_series/VVP_kvartal_s 1995-2023.xlsx\", \"2\")[2:]\n",
    "GDP = GDP.iloc[1]\n",
    "\n",
    "offset = 4 * (2016-2011)\n",
    "GDP = GDP[offset:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['quartal'] = new_df.index.month // 4 + 1\n",
    "new_df['quartal'] = (new_df.index.year - 2016)*4 + new_df['quartal']\n",
    "\n",
    "new_df['GDP_by_prev_quartal'] = new_df['quartal'].apply(lambda quartal: GDP[quartal - 2])\n",
    "\n",
    "new_df.drop(columns='quartal', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ключевая ставка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyRates = pd.read_csv('/home/owner/Documents/DEV/Time_series/KeyRate.csv', header=None, names=['dates', 'rates'],  sep=';', index_col='dates')\n",
    "keyRates.index = pd.to_datetime(keyRates.index)\n",
    "keyRates.sort_index(inplace=True)\n",
    "new_df['key_rate'] = new_df.index.to_series().apply(lambda date: keyRates.rates[keyRates.index < date][-1])\n",
    "# здесь уже со сдвигом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Праздничные дни"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_holidays = pd.to_datetime(np.fromiter(holidays.RU(years=[2017, 2018, 2019, 2020, 2021]).keys(), 'datetime64[ns]'))\n",
    "all_holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['holiday'] = new_df.index.isin(all_holidays).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.holiday.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_holiday = new_df.holiday.astype(bool)\n",
    "new_df['week_before_holiday'] = bool_holiday.shift(-1).fillna(0)| \\\n",
    "    bool_holiday.shift(-2).fillna(0) | bool_holiday.shift(-3).fillna(0) | \\\n",
    "    bool_holiday.shift(-4).fillna(0) | bool_holiday.shift(-5).fillna(0) | \\\n",
    "    bool_holiday.shift(-6).fillna(0) | bool_holiday.shift(-7).fillna(0)\n",
    "new_df.week_before_holiday = new_df.week_before_holiday.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['day_before_holiday'] = new_df.holiday.shift(-1).fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Мб еще отдельно обработаем даты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Типо выделим месяцы и заэнкодим мб времена года"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = new_df.index.month\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['spring'] = new_df.index.month.isin([3,4,5]).astype(int)\n",
    "new_df['summer'] = new_df.index.month.isin([6,7,8]).astype(int)\n",
    "new_df['autumn'] = new_df.index.month.isin([9,10,11]).astype(int)\n",
    "new_df['winter'] = new_df.index.month.isin([12,1,2]).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сделаем энкодинг дней недели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['weekdays'] = new_df.index.day_name()\n",
    "new_df = new_df.join(new_df['weekdays'].str.get_dummies()).drop(columns='weekdays')\n",
    "for weekday in new_df.index.day_name().unique(): new_df[weekday] = new_df[weekday].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['month'] = new_df.index.month_name()\n",
    "new_df = new_df.join(new_df['month'].str.get_dummies()).drop(columns='month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Средние значения таргета в каждом месяце"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = new_df.index.month_name().unique()\n",
    "new_df['avg_Balance_by_this_month'] = 0\n",
    "for month in months:\n",
    "    mask = new_df[month].astype(bool)\n",
    "    new_df[mask]['avg_Balance_by_this_month'] = new_df[mask]['Balance'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выходные в отдельный признак"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['weekend'] = (new_df.Saturday | new_df.Sunday).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Средние значения таргета по выходным"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = new_df.weekend.astype(bool)\n",
    "new_df[mask]['avg_Balance_weekend'] = new_df[mask | mask_before]['Balance'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Средние значения по временам года"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons = ['winter', 'spring', 'summer', 'autumn']\n",
    "new_df['avg_Balance_by_season'] = 0\n",
    "for season in seasons:\n",
    "    mask = new_df[season].astype(bool)\n",
    "    new_df[mask]['avg_Balance_by_season'] = new_df[mask | mask_before]['Balance'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Средние значения по неделе месяца"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[mask | mask_before]['avg_Balance_by_week_of_month'] = 0\n",
    "\n",
    "mask = new_df.index.day <= 7\n",
    "new_df[mask]['avg_Balance_by_week_of_month'] = new_df[mask | mask_before]['Balance'].mean()\n",
    "mask = (new_df.index.day >= 8) & (new_df.index.day <= 14) \n",
    "new_df[mask]['avg_Balance_by_week_of_month'] = new_df[mask | mask_before]['Balance'].mean()\n",
    "mask = (new_df.index.day >= 15) & (new_df.index.day <= 21) \n",
    "new_df[mask]['avg_Balance_by_week_of_month'] = new_df[mask | mask_before]['Balance'].mean()\n",
    "mask = new_df.index.day >= 22\n",
    "new_df[mask]['avg_Balance_by_week_of_month'] = new_df[mask | mask_before]['Balance'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пусть будет признак четности года"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['even_year'] = (~(new_df.index.year % 2).astype(bool)).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Средние по дням недели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['avg_Balance_by_day'] = 0\n",
    "for week_day in new_df.index.day_name():\n",
    "    mask = new_df[week_day].astype(bool)\n",
    "    new_df[mask]['avg_Balance_by_season'] = new_df[mask | mask_before]['Balance'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Средняя по кварталам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[mask]['avg_Balance_by_quartal'] = 0\n",
    "\n",
    "mask = new_df.index.month.isin([1,2,3])\n",
    "new_df[mask]['avg_Balance_by_quartal'] = new_df[mask | mask_before]['Balance'].mean()\n",
    "mask = new_df.index.month.isin([4,5,6])\n",
    "new_df[mask]['avg_Balance_by_quartal'] = new_df[mask | mask_before]['Balance'].mean()\n",
    "mask = new_df.index.month.isin([7,8,9])\n",
    "new_df[mask]['avg_Balance_by_quartal'] = new_df[mask | mask_before]['Balance'].mean()\n",
    "mask = new_df.index.month.isin([10,11,12])\n",
    "new_df[mask]['avg_Balance_by_quartal'] = new_df[mask | mask_before]['Balance'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Налоговые дни"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для ЕНС установлены единые сроки: до 25 числа месяца, следующего за отчетным периодом, нужно отчитаться перед налоговой, а до 28 числа — уплатить налоги."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['tax_day'] = new_df.index.day.isin([28]).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Экспоненциальное сглаживание"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_smoothing = [new_df.Balance.iloc[0]]\n",
    "\n",
    "alpha = 0.2\n",
    "\n",
    "for i in range(1, len(new_df)):\n",
    "    exp_smoothing.append(alpha * new_df.Balance.iloc[i] + (1 - alpha) * exp_smoothing[i - 1])\n",
    "    \n",
    "new_df['exp_smoothing'] = exp_smoothing\n",
    "new_df.exp_smoothing = new_df.exp_smoothing.shift(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[['Balance', 'exp_smoothing']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(new_df.Balance, color='red')\n",
    "sns.lineplot(new_df.exp_smoothing, color = 'green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сдвиги и rolling window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_window_stats(window_size):\n",
    "    new_df[f'min_last_{window_size}'] = new_df.Balance.rolling(7).min().shift(1)\n",
    "    new_df[f'max_last_{window_size}'] = new_df.Balance.rolling(7).max().shift(1)\n",
    "    new_df[f'mean_last_{window_size}'] = new_df.Balance.rolling(7).mean().shift(1)\n",
    "    \n",
    "    last_min = np.inf\n",
    "    last_max = - np.inf\n",
    "    sum = 0\n",
    "\n",
    "    new_df[f'min_last_{window_size}'].iloc[0] = 0\n",
    "    new_df[f'max_last_{window_size}'].iloc[0] = 0\n",
    "    new_df[f'mean_last_{window_size}'].iloc[0] = 0\n",
    "\n",
    "    for i in range(1,window_size):\n",
    "        if last_min > new_df.Balance.iloc[i-1]:\n",
    "            last_min = new_df.Balance.iloc[i-1]\n",
    "        new_df[f'min_last_{window_size}'].iloc[i] = last_min\n",
    "        \n",
    "        if last_max < new_df.Balance.iloc[i-1]:\n",
    "            last_max = new_df.Balance.iloc[i-1]\n",
    "        new_df[f'max_last_{window_size}'].iloc[i] = last_max\n",
    "        \n",
    "        sum += new_df.Balance.iloc[i-1]\n",
    "        new_df[f'mean_last_{window_size}'].iloc[i] = sum / i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_window_stats(7)\n",
    "rolling_window_stats(14)\n",
    "rolling_window_stats(21)\n",
    "rolling_window_stats(28)\n",
    "rolling_window_stats(31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_shifts = 8\n",
    "for i in range(1, n_shifts+1):\n",
    "    new_df[f'shift_{i}'] = new_df.Balance.shift(i)\n",
    "new_df = new_df[n_shifts:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обновим разбиение относительно new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_before = (new_df.index < threshold_date) & (new_df.index >= start_date)\n",
    "mask_after = (new_df.index >= threshold_date) & (new_df.index <= end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Дальше будут пункты с экзогенными факторами, поэтому выделим их заранее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df_train = new_df[new_df.index < threshold_date]\n",
    "# new_df_test = new_df[mask_after]\n",
    "exog = list(filter(lambda column: column != 'Balance' and not column.startswith(\"shift\"), new_df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Регрессор со внутренним отбором признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/expand-your-time-series-arsenal-with-these-models-10c807d37558"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.relativedelta import relativedelta\n",
    "from scalecast.Forecaster import Forecaster\n",
    "from scalecast import GridGenerator\n",
    "from scalecast.auxmodels import mlp_stack\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_test_export_summaries(f):\n",
    "    \"\"\" exports the relevant statisitcal information and displays a plot of the test-set results for the last model run\n",
    "    \"\"\"\n",
    "    f.plot_test_set(models=f.estimator,ci=True)\n",
    "    plt.title(f'{f.estimator} test-set results',size=16)\n",
    "    plt.show()\n",
    "    return f.export('model_summaries',determine_best_by='TestSetMAE')[\n",
    "        [\n",
    "            'ModelNickname',\n",
    "            'HyperParams',\n",
    "            'TestSetMAE',\n",
    "            'TestSetMAXAE',\n",
    "            'InSampleMAE',\n",
    "            'InSampleMAXAE'\n",
    "        ]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=Forecaster(y=new_df.Balance,current_dates=new_df.index)\n",
    "f.plot()\n",
    "plt.title('Orig Series',size=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scalecast.util import metrics\n",
    "def maxae(a,f):\n",
    "    # average of rmse and mae\n",
    "    return np.max(np.array(a)-np.array(f))\n",
    "f.add_metric(maxae, 'MAXAE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst_length = 60\n",
    "f.generate_future_dates(fcst_length)\n",
    "f.set_test_length(.2)\n",
    "f.eval_cis() # tell the object to build confidence intervals for all models\n",
    "f.add_ar_terms(7)\n",
    "f.add_AR_terms((4,7))\n",
    "f.add_seasonal_regressors('month','quarter','week','dayofyear',raw=False,sincos=True)\n",
    "f.add_seasonal_regressors('dayofweek','is_leap_year','week',raw=False,dummy=True,drop_first=True)\n",
    "f.add_seasonal_regressors('year')\n",
    "f.set_validation_metric('mae')\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.set_estimator('lasso')\n",
    "lasso_grid = {'alpha':np.linspace(0,2,100)}\n",
    "f.ingest_grid(lasso_grid)\n",
    "f.cross_validate(k=3)\n",
    "f.auto_forecast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_test_export_summaries(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.set_estimator('ridge')\n",
    "f.ingest_grid(lasso_grid)\n",
    "f.cross_validate(k=3)\n",
    "f.auto_forecast()\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_test_export_summaries(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.set_estimator('elasticnet')\n",
    "f.ingest_grid(lasso_grid)\n",
    "f.cross_validate(k=3)\n",
    "f.auto_forecast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_test_export_summaries(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пытаемся замутить SARIMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "# Define the range of values for p, d, q, P, D, Q, and m\n",
    "p_values = range(0, 4)  # Autoregressive order\n",
    "d_values = [0]          # Differencing order\n",
    "q_values = range(0, 4)  # Moving average order\n",
    "P_values = range(0, 5) # Seasonal autoregressive order\n",
    "D_values = range(0,2) # Seasonal differencing order\n",
    "Q_values = range(0, 3)  # Seasonal moving average order\n",
    "m_values = [7]         # Seasonal period\n",
    "\n",
    "# Create all possible combinations of SARIMA parameters\n",
    "param_combinations = list(itertools.product(p_values, \n",
    "                                            d_values, \n",
    "                                            q_values, \n",
    "                                            P_values, \n",
    "                                            D_values, \n",
    "                                            Q_values, \n",
    "                                            m_values))\n",
    "results_exog = []\n",
    "\n",
    "# Initialize AIC with a large value\n",
    "best_aic_SARIMAX = float(\"inf\")  \n",
    "best_params_SARIMAX = None\n",
    "best_SARIMA = None\n",
    "mask_before = (new_df.index < threshold_date) & (new_df.index >= start_date)\n",
    "mask_after = (new_df.index >= threshold_date) & (new_df.index <= end_date)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Perform grid search\n",
    "for params in tqdm(param_combinations):\n",
    "    order = params[:3]\n",
    "    seasonal_order = params[3:]\n",
    "    \n",
    "    try:\n",
    "        model_chosen = ARIMA(endog=new_df[mask_before].Balance,\n",
    "                               exog=new_df[mask_before][exog],\n",
    "                               order=order, \n",
    "                               seasonal_order=seasonal_order)\n",
    "        model_chosen = model_chosen.fit()\n",
    "        aic = model_chosen.aic\n",
    "        \n",
    "        # Ensure the convergence of the model\n",
    "        if not math.isinf(model_chosen.zvalues.mean()):\n",
    "            predict=model_chosen.predict(start=threshold_date, end=end_date, exog=new_df[mask_after][exog])\n",
    "            err_max = (new_df.Balance[mask_after]-predict).abs().max()\n",
    "            mae = mean_absolute_error(new_df.Balance[mask_after], predict)\n",
    "            llf = model_chosen.llf\n",
    "            r2 = r2_score(new_df.Balance[mask_after], predict)\n",
    "            aic = model_chosen.aic\n",
    "            if aic < best_aic_SARIMAX:\n",
    "                best_aic_SARIMAX = aic\n",
    "                best_params_SARIMAX = params\n",
    "                best_SARIMAX = model_chosen\n",
    "                best_err_max = err_max\n",
    "                best_mae = mae\n",
    "                best_llf = llf\n",
    "                best_r2 = r2\n",
    "                best_combination = params\n",
    "                best_aic_SARIMAX = aic\n",
    "            results_exog.append(list(params[:-1]) + [err_max, mae, llf, r2, aic])\n",
    "        else:\n",
    "            print(order, seasonal_order, 'not converged')\n",
    "    except ValueError as err:\n",
    "        print('wrong parameters:', params)  \n",
    "        print(err.args)\n",
    "        continue\n",
    "\n",
    "# Print the best parameters and AIC\n",
    "print(\"Best Parameters:\", best_params_SARIMAX)\n",
    "print(\"Best AIC:\", best_aic_SARIMAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_exog_table = pd.DataFrame(results_exog)\n",
    "result_exog_table.columns = ['p', 'd', 'q', 'P', 'D', 'Q', 'err_max', 'MAE', 'LLF', 'R2', 'aic']\n",
    "print(\"by LLF:\")\n",
    "display(result_exog_table.sort_values(by=['LLF'], ascending=False).head(1))\n",
    "print(\"by err_max:\")\n",
    "display(result_exog_table.sort_values(by=['err_max'], ascending=True).head(1))\n",
    "print(\"by MAE:\")\n",
    "display(result_exog_table.sort_values(by=['MAE'], ascending=True).head(1))\n",
    "print(\"by aic:\")\n",
    "display(result_exog_table.sort_values(by=['aic'], ascending=True).head(1))\n",
    "print(\"by R2:\")\n",
    "display(result_exog_table.sort_values(by=['R2'], ascending=False).head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = best_SARIMAX.params\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(np.abs(importance)):\n",
    " print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# plot feature importance\n",
    "plt.bar([x for x in range(len(importance))], np.abs(importance))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(result_exog_table.sort_values(by=['MAE'], ascending=True).head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SARIMAX_by_MAE = ARIMA(endog=new_df[mask_before].Balance,\n",
    "                               exog=new_df[mask_before][exog],\n",
    "                               order=(1,0,1), \n",
    "                               seasonal_order=(4,1,2,7)).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(result_exog_table.sort_values(by=['err_max'], ascending=True).head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SARIMAX_by_err_max = ARIMA(endog=new_df[mask_before].Balance,\n",
    "                               exog=new_df[mask_before][exog],\n",
    "                               order=(3, 0, 3), \n",
    "                               seasonal_order=(2,1,1,7)).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_SARIMAX = best_SARIMAX.predict(start=threshold_date, end=end_date, exog=new_df[mask_after][exog])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_SARIMAX_MAE = SARIMAX_by_MAE.predict(start=threshold_date, end=end_date, exog=new_df[mask_after][exog])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_SARIMAX_err_max = SARIMAX_by_err_max.predict(start=threshold_date, end=end_date, exog=new_df[mask_after][exog])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=new_df.index, y=new_df.Balance, label='Balance', color = 'purple')\n",
    "sns.lineplot(x=new_df.index[mask_after], y=-predict_SARIMAX, label='pred_chosen', linestyle='dashed', color='red')\n",
    "# sns.lineplot(x=new_df.index[mask_after], y=predict_SARIMAX_MAE, label='by_MAE', color='darkgreen')\n",
    "# sns.lineplot(x=new_df.index[mask_after], y=predict_SARIMAX_err_max, label='by_err_max', linestyle='dashed', color='black')\n",
    "plt.axvline(x=threshold_date)\n",
    "# sns.lineplot(new_df.exp_smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsplot(SARIMAX_by_err_max.resid, lags=30) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsplot(SARIMAX_by_MAE.resid, lags=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsplot(best_SARIMAX.resid, lags=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_SARIMAX.plot_diagnostics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "arimax_resid = best_SARIMAX.resid\n",
    "white_noise_arima = acorr_ljungbox(arimax_resid, lags=None, return_df=True)\n",
    "white_noise_arima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все p-value огромные, мы не можем отвергнуть нулевую гипотезу о независимости остатков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_test = sm.stats.diagnostic.het_arch(best_SARIMAX.resid**2)\n",
    "display(f'ARCH test results:\\n')\n",
    "display(f'LM Statistic: {arch_test[0]}')\n",
    "display(f'p-value: {arch_test[1]}')\n",
    "display(f'F Statistic: {arch_test[2]}')\n",
    "display(f'p-value: {arch_test[3]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вроде как p-value большеват, ARCH эффект нельзя доказать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsplot(best_SARIMAX.resid**2, lags=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arch import arch_model\n",
    "\n",
    "ps = range(1,15)\n",
    "qs = range(1,15)\n",
    "\n",
    "params_variants = list(itertools.product(ps, qs))\n",
    "results = []\n",
    "\n",
    "for p, q in tqdm(params_variants):\n",
    "    amx = arch_model(best_SARIMAX.resid, p=p, q=q).fit()\n",
    "    \n",
    "    result = {\n",
    "        'p': p,\n",
    "        'q': q,\n",
    "        'AIC': amx.aic,\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "best_model_aic = results_df.loc[results_df['AIC'].idxmin()]\n",
    "\n",
    "display(\"Best GARCH model (AIC):\")\n",
    "display(best_model_aic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 11\n",
    "q = 8\n",
    "amx = arch_model(best_SARIMAX.resid, p=p, q=q).fit()\n",
    "fig = amx.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsplot(amx.resid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "garch_resid = amx.resid\n",
    "white_noise_garch = acorr_ljungbox(garch_resid, lags = [10], return_df=True)\n",
    "white_noise_garch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_volatility = am.forecast(start=threshold_date-1, horizon=pd.Timedelta(end_date-threshold_date).days+1)\n",
    "forecast_volatility_values = forecast_volatility.variance.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_bound = predict_chosen + np.sqrt(forecast_volatility_values)\n",
    "lower_bound = predict_chosen - np.sqrt(forecast_volatility_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "sns.lineplot(x=new_df.index[mask_after], y=predict_SARIMAX, label='pred_chosen', linestyle='dashed', color='red', ax=ax)\n",
    "# sns.lineplot(x=df.index[mask_after], y=(upper_bound + lower_bound)/2, label='maybe', color = 'magenta')\n",
    "\n",
    "sns.lineplot(x=new_df.index[mask_after], y=upper_bound, label='bounds', color = 'magenta')\n",
    "sns.lineplot(x=new_df.index[mask_after], y=lower_bound, label='bounds', color = 'magenta')\n",
    "sns.lineplot(x=new_df.index[mask_after], y=new_df.Balance[mask_after], label='test', color = 'darkgreen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Holt-Winters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# holt winters \n",
    "# single exponential smoothing\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing   \n",
    "# double and triple exponential smoothing\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.index.freq = 'D'\n",
    "# Set the value of Alpha and define m (Time Period)\n",
    "m = 7\n",
    "alpha = 1/(2*m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HWES1 = SimpleExpSmoothing(new_df['Balance'][mask_before]).fit(smoothing_level=alpha,optimized=False,use_brute=True).fittedvalues\n",
    "# new_df[['Balance']][mask_before].plot(title='Holt Winters Single Exponential Smoothing'),\n",
    "# sns.lineplot(HWES1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HWES2_ADD = ExponentialSmoothing(new_df['Balance'][mask_before],trend='add').fit().fittedvalues\n",
    "\n",
    "# new_df[['Balance'][mask_before].plot(title='Holt Winters Double Exponential Smoothing: Additive Trend');\n",
    "# sns.lineplot(HWES2_ADD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['posBalance'] = new_df.Balance + np.abs(new_df.Balance.min()) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HW_model = ExponentialSmoothing(new_df['Balance'][mask_before],trend='add',seasonal='add',seasonal_periods=7).fit()\n",
    "new_df['HWES3_ADD'] = HW_model.fittedvalues\n",
    "\n",
    "new_df[['Balance','HWES3_ADD']][mask_before].plot(title='Holt Winters Triple Exponential Smoothing: Additive Seasonality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HW_predict = HW_model.forecast(steps=int((end_date - threshold_date).days)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(new_df.Balance)\n",
    "sns.lineplot(HW_model.fittedvalues)\n",
    "sns.lineplot(HW_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = new_df.drop(columns=['Balance', 'HWES3_ADD'])\n",
    "y = new_df.Balance\n",
    "\n",
    "X_train, X_test = X[mask_before], X[mask_after]\n",
    "y_train, y_test = y[mask_before], y[mask_after]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_SK = [\n",
    "    {\n",
    "        'n_estimators': [300, 1000], \n",
    "        'min_samples_leaf': [1, 5, 10, 20, 50], \n",
    "        'max_depth': [3, 6, 12],\n",
    "        'max_features': ['sqrt', 'log2', .01, .5, .1, .5, .9],\n",
    "        'learning_rate' : [.1, .3, .5, .9], \n",
    "        'loss': ['absolute_error']\n",
    "    },\n",
    "    {\n",
    "        'n_estimators': [300, 1000], \n",
    "        'min_samples_leaf': [1, 5, 10, 20, 50], \n",
    "        'alpha': [0.09] + list(np.logspace(-5, 0, 10, endpoint=True)), \n",
    "        'max_depth': [3, 6, 12],\n",
    "        'max_features': ['sqrt', 'log2', .01, .5, .1, .5, .9],\n",
    "        'learning_rate' : [.1, .3, .5, .9], \n",
    "        'loss': ['quantile']\n",
    "    }\n",
    "]\n",
    "param_grid_XGB = [\n",
    "    {\n",
    "        'n_estimators' : [300, 1000], \n",
    "        'learning_rate' : [.1, .3, .5, .9], \n",
    "        'max_depth': [3, 6, 12], \n",
    "        'min_child_weight': [1, 3, 5, 10],\n",
    "        'grow_policy': ['depthwise', 'losswise'],\n",
    "        'objective': ['reg:absoluteerror']\n",
    "     },\n",
    "    {\n",
    "        'n_estimators' : [300, 1000], \n",
    "        'learning_rate' : [.1, .3, .5, .9], \n",
    "        'quantile_alpha' : [0.09] + list(np.logspace(-5, 0, 10, endpoint=True)), \n",
    "        'max_depth': [3, 6, 12], \n",
    "        'min_child_weight': [1, 3, 5, 10],\n",
    "        'grow_policy': ['depthwise', 'losswise'],\n",
    "        'objective': ['reg:quantileerror']\n",
    "     }\n",
    "]\n",
    "param_grid_LGBM = [\n",
    "    {\n",
    "        'n_estimators' : [300, 1000], \n",
    "        'learning_rate' : [.1, .3, .5, .9], \n",
    "        'max_depth': [3, 6, 12], \n",
    "        'min_child_samples': [1, 5, 10, 20, 50],\n",
    "        'num_leaves': [10, 31, 50, 100],\n",
    "        'objective': ['mae'],\n",
    "        'njobs': [10]\n",
    "     },\n",
    "    {\n",
    "        'n_estimators' : [300, 1000], \n",
    "        'learning_rate' : [.1, .3, .5, .9], \n",
    "        'quantile_alpha' : [0.09] + list(np.logspace(-5, 0, 10, endpoint=True)), \n",
    "        'min_child_samples': [1, 5, 10, 20, 50],\n",
    "        'max_depth': [3, 6, 12], \n",
    "        'num_leaves': [10, 31, 50, 100],\n",
    "        'objective': ['quantile'],\n",
    "        'njobs': [10]\n",
    "     }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from pactools.grid_search import GridSearchCVProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "# from tqdm import tqdm if not in notebook\n",
    "# https://datascience.stackexchange.com/questions/114060/progress-bar-for-gridsearchcv\n",
    "def fit(model, *args, **kwargs):\n",
    "    class BarStdout:\n",
    "        def write(self, text):\n",
    "            if \"totalling\" in text and \"fits\" in text:\n",
    "                self.bar_size = int(text.split(\"totalling\")[1].split(\"fits\")[0][1:-1])\n",
    "                self.bar = tqdm(range(self.bar_size))\n",
    "                self.count = 0\n",
    "                return\n",
    "            if \"CV\" in text and hasattr(self,\"bar\"):\n",
    "                self.count += 1\n",
    "                self.bar.update(n=self.count-self.bar.n)\n",
    "                if self.count%(self.bar_size//10)==0:\n",
    "                    time.sleep(0.1)\n",
    "        def flush(self, text=None):\n",
    "            pass\n",
    "    default_stdout= sys.stdout\n",
    "    sys.stdout = BarStdout()\n",
    "    model.verbose = 10\n",
    "    model.fit(*args, **kwargs)\n",
    "    sys.stdout = default_stdout\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridSK = GridSearchCVProgressBar(GradientBoostingRegressor(), \n",
    "#                        param_grid_SK, \n",
    "#                        scoring='neg_mean_absolute_error',\n",
    "#                        cv=5)\n",
    "# modelSK = fit(gridSK, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridXGB = GridSearchCVProgressBar(XGBRegressor(), \n",
    "#                        param_grid_XGB, \n",
    "#                        scoring='neg_mean_absolute_error',\n",
    "#                        cv=5).fit(X_train, y_train)\n",
    "# modelXGB = fit(gridXGB, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridLGBM = GridSearchCVProgressBar(LGBMRegressor(), \n",
    "#                        param_grid_LGBM, \n",
    "#                        scoring='neg_mean_absolute_error',\n",
    "#                        cv=5).fit(X_train, y_train)\n",
    "# modelLGBM = fit(gridLGBM, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelSK.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelXGB.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelLGBM.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelSK = GradientBoostingRegressor(learning_rate=0.1, loss='absolute_error', max_depth=3, max_features=0.9, min_samples_leaf=20, n_estimators=300).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelXGB = XGBRegressor(grow_policy='depthwise', learning_rate=0.1, max_depth=3, min_child_weight=10, n_estimators=300, objective='reg:absoluteerror').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelLGBM = LGBMRegressor(learning_rate=0.1, max_depth=3, min_child_samples=50, n_estimators=300, njobs=10, num_leaves=10, objective='mae').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictSK = modelSK.predict(X_test)\n",
    "predictXGB = modelXGB.predict(X_test)\n",
    "predictLGBM = modelLGBM.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(f\"mean scikit = {(new_df.Balance[mask_after]-predictSK).abs().mean()}\")\n",
    "display(f\"mean XGB = {(new_df.Balance[mask_after]-predictXGB).abs().mean()}\")\n",
    "display(f\"mean LGBM = {(new_df.Balance[mask_after]-predictLGBM).abs().mean()}\")\n",
    "display(f\"max scikit = {(new_df.Balance[mask_after]-predictSK).abs().max()}\")\n",
    "display(f\"max XGB = {(new_df.Balance[mask_after]-predictXGB).abs().max()}\")\n",
    "display(f\"max LGBM = {(new_df.Balance[mask_after]-predictLGBM).abs().max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axvline(x=threshold_date, color='magenta')\n",
    "sns.lineplot(x=new_df.index, y=new_df.Balance, label='test', color = 'purple')\n",
    "# sns.lineplot(x=new_df.index[mask_after], y=predictSK, label='sklearn', color='red')\n",
    "sns.lineplot(x=new_df.index[mask_after], y=predictLGBM, label='LGBM', color='blue')\n",
    "# sns.lineplot(x=new_df.index[mask_after], y=predictXGB, label='XGB', color='darkgreen')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
